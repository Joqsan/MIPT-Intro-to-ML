{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Recap Lecture 1.\n",
    "\n",
    "\n",
    "- Supervised Learning: target is available.\n",
    "    - Classification.\n",
    "    - Regression.\n",
    "\n",
    "- Unsupervised Learning: without target.\n",
    "    - Dimensionality Reduction.\n",
    "    - Clustering.\n",
    "\n",
    "----\n",
    "\n",
    "# 1. Linear Models.\n",
    "\n",
    "They are used for supervised (regression and classification) and unsupervised.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1. For Regression.\n",
    "\n",
    "![alt text](https://i.ibb.co/NtdL1Jn/Screen-Shot-2020-09-15-at-02-01-58.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2. For Classification.\n",
    "\n",
    "![alt text](https://i.ibb.co/BjXw8yt/Screen-Shot-2020-09-15-at-02-03-43.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3. For PCA.\n",
    "\n",
    "We perform dimensionality reduction, losing some information by using some linear combination of the original features in order to generate new (in a lower dimensional space) ones.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4. As Building Block.\n",
    "\n",
    "They can be used as building block for other models, like Ensemble, NN, etc.\n",
    "\n",
    "![alt text](https://i.ibb.co/bvY1H0n/Screen-Shot-2020-09-15-at-02-08-48.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Linear Regression.\n",
    "\n",
    "Let's discuss first what a regression model is about.\n",
    "\n",
    "- X, Y - random variables. \n",
    "- $\\varepsilon$ - error. Random noise with $\\mathbb{E}\\varepsilon = 0$.\n",
    "\n",
    "![alt text](https://i.ibb.co/nMTqK67/Screen-Shot-2020-09-15-at-02-11-41.png)\n",
    "\n",
    "To get linear regression from  just regression by taking $f(X)$ to be a linear function.\n",
    "\n",
    "![alt text](https://i.ibb.co/qB1Yn5m/Screen-Shot-2020-09-15-at-02-15-25.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1. Linear Regression: Problem Statement.\n",
    "\n",
    "![alt text](https://i.ibb.co/pyVMRK2/Screen-Shot-2020-09-15-at-02-17-11.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.2. How to choose weights.\n",
    "\n",
    "![alt text](https://i.ibb.co/bXbPTSd/Screen-Shot-2020-09-15-at-02-23-01.png)\n",
    "\n",
    "When the loss is MSE, then the optimization problem the the error $Q(X)$ has analytical solution:\n",
    "\n",
    "![alt text](https://i.ibb.co/7C3Dqpr/Screen-Shot-2020-09-15-at-02-24-29.png)\n",
    "\n",
    "There good and bad news about it.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.3. Good News: Gauss-Markov Theorem.\n",
    "\n",
    "![alt text](https://i.ibb.co/gg171Rx/Screen-Shot-2020-09-15-at-02-29-12.png)\n",
    "\n",
    "\n",
    "- Statement: The Gauss–Markov theorem states that the ordinary least-squares estimator $\\hat{w} = (X^{\\top}X)^{-1}X^{\\top}y$ of the parameter $w$ in the linear regression model $y = Xw + \\varepsilon$ is the unbiased linear estimator with minimum variance.\n",
    "- **In Short:** OLS is BLUE.\n",
    "\n",
    "- unbiased estimator: This is when:\n",
    "    - $\\mathrm{Bias}(\\hat{w}, w) = \\mathbb{E}[\\hat{w} \\mid X] - w = 0$, or\n",
    "    - $\\mathbb{E}[\\hat{w} \\mid X] = w$, вот это называется **<font color=red>несмещенной оценкой</font>**.\n",
    "    - Read more [here](https://www.le.ac.uk/users/dsgp1/COURSES/TOPICS/gausmkov.pdf) and ([here](https://www.econometrics-with-r.org/4-4-tlsa.html), [here](https://www.econometrics-with-r.org/5-5-the-gauss-markov-theorem.html)).\n",
    "\n",
    "Fro the equation above:\n",
    "\n",
    "- $Y$ is random.\n",
    "- $X$ is usually random (since we didn't choose it).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1.4. Bad News: Instability.\n",
    "\n",
    "![alt text](https://i.ibb.co/cXmPPyw/Screen-Shot-2020-09-15-at-14-42-26-1.png)\n",
    "\n",
    "- The sum of the coefficients in both cases is the same.\n",
    "\n",
    "Как с этим бороться?\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1.4.1. L2 Regularization.\n",
    "\n",
    "![alt text](https://i.ibb.co/6vRVb3j/Screen-Shot-2020-09-15-at-14-50-02.png)\n",
    "\n",
    "**Fact 1:**\n",
    "\n",
    "![alt text](https://i.ibb.co/6BnXSZ6/Screen-Shot-2020-09-15-at-14-54-54.png)\n",
    "\n",
    "(source [here](https://arxiv.org/pdf/1509.09169;Lecture))\n",
    "\n",
    "By **THEOREM 1.2.** we can find some $\\lambda_0 > 0$, such that estimator $\\hat{w}_0 = (X^{\\top}X + \\lambda_0^2I)^{-1}X^{\\top}y$ outperforms (in terms of MSE) the estimator $\\hat{w} = (X^{\\top}X)^{-1}X^{\\top}y$. If $\\hat{w}_0$ is unbiased, then it contradicts the Gauss-Markov Theorem, that says that $\\hat{w}$ is BLUE. Therefore, $\\hat{w}_0$ has to be biased (не является несмещенной).\n",
    "\n",
    "- With regularization there is more stability.\n",
    "- As regularization ($\\lambda$) increases, the weights values decrease.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1.4.2. L1 Regularization.\n",
    "\n",
    "- **<font color=red>Тут $w$ остала $\\theta$</font>**.\n",
    "- Blue line, original model weight.\n",
    "- Orange line, with L1 regularization.\n",
    "    - В функции не $\\lambda$ а $\\lambda^2$.\n",
    "\n",
    "\n",
    "![alt text](https://i.ibb.co/31FQtSP/Screen-Shot-2020-09-15-at-15-15-58.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1.4.3. Elastic Net.\n",
    "\n",
    "![alt text](https://i.ibb.co/jfTqcnF/Screen-Shot-2020-09-15-at-15-28-26.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Metric in Regression.\n",
    "\n",
    "![alt text](https://i.ibb.co/Nn4PSfx/Screen-Shot-2020-09-15-at-15-29-45.png)\n",
    "\n",
    "- $R^2$ мы сравняем нашу модель с той, которая одну и ту же константу предсказует для каждой точки.\n",
    "    - $\\bar{y}$ is the constant that minimizes the constant model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 4. Model Building Cycle.\n",
    "\n",
    "\n",
    "![alt text](https://i.ibb.co/0yQ1f51/Screen-Shot-2020-09-15-at-15-41-59.png)\n",
    "\n",
    "- We check for hyperparameter values on the validation set.\n",
    "- We check for metric values on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 5. Overfitting: Runge's Phenomenon.\n",
    "\n",
    "![alt text](https://i.ibb.co/qjxp935/Screen-Shot-2020-09-15-at-15-53-53.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
