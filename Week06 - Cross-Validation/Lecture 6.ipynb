{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Test Review.\n",
    "\n",
    "\n",
    "1, Margin: \n",
    "- All clear.\n",
    "\n",
    "\n",
    "2, SVM:\n",
    "\n",
    "- doesn't delete outliers.\n",
    "- maximizes street width between two classes.\n",
    "- $L_1$ regularization doesn't come up from the problem statement.\n",
    "- doesn't perform feature selection\n",
    "\n",
    "3, Kernel Trick:\n",
    "\n",
    "- separate classes using nonlinear border between classes.\n",
    "- doesn't speeds up the convergence of the optimization procedure.\n",
    "- doesn't selects the optimal transformation of the scalar product automatically, since it depends on the kernel chosen before training.\n",
    "- can be used in kNN (by some means) as well.\n",
    "\n",
    "4, PCA:\n",
    "\n",
    "- allows to find a linear mapping from original features space to subspace of lower dimensionality\n",
    "- appliying PCA of $k$ features to dataset already described by $k$ features won't return the dataset itself.\n",
    "    - This is because the resulting decomposition gives $T_{(n\\times k)} = X_{(n\\times k)}W_{(k\\times k)}$ where $W$ is the matrix of right singular vector of $X$. If $W \\neq E$, then $T \\neq X$.\n",
    "- can't find linear and nonlinear combinations of the features.\n",
    "    - It only allows to go to lower dimensions.\n",
    "- can be only applied to data seen at the moment it was 'fitted', so we can not apply the same transformation to the new data.\n",
    "    - This is false. PCA can be apply to any matrix.\n",
    "    \n",
    "5, What PCA and kNN have in common?\n",
    "\n",
    "- They both require normalized data (unless you know that in this particular case they don't).\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Validation.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1. KFold.\n",
    "\n",
    "![alt text](https://i.ibb.co/K0G4ZcB/Screen-Shot-2020-10-01-at-12-54-40.png)\n",
    "\n",
    "We\n",
    "- leave one fold for validation.\n",
    "- train in the remaining k-! folds.\n",
    "- prediction are done my averagind each fold's prediction on the validation set.\n",
    "\n",
    "Drawback:\n",
    "- Not good if data is ordered: one or more folds may end up having instances from just one class, which makes the model generalize poorly.\n",
    "\n",
    "How to avoid this problem: StratifiedShuffleSplit.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2. ShuffleSplit.\n",
    "\n",
    "\n",
    "![alt text](https://i.ibb.co/qWgSVps/Screen-Shot-2020-10-01-at-12-55-04.png)\n",
    "\n",
    "- Shuffle the data once and then split.\n",
    "- There is no care for preserving the frequency each class appears in the data with.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3. StratifiedKFold.\n",
    "\n",
    "![alt text](https://i.ibb.co/6JsDPKb/Screen-Shot-2020-10-01-at-13-04-22.png)\n",
    "\n",
    "- The folds are made by preserving the percentage of samples for each class.\n",
    "- The data is shuffled once at the start, and then divided into the number of desired splits. The test data is always one of the splits, the train data is the rest.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4. StratifiedShuffleSplit.\n",
    "\n",
    "![alt text](https://i.ibb.co/mX3VqZZ/Screen-Shot-2020-10-01-at-13-13-12.png)\n",
    "\n",
    "- The folds are made by preserving the percentage of samples for each class (it preserves the frequency each class appears in the data).\n",
    "- The data is shuffled every time, and then split. This means the test sets may overlap between the splits.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5.  GroupKFold.\n",
    "\n",
    "![alt text](https://i.ibb.co/2YxYmXf/Screen-Shot-2020-10-01-at-13-16-16.png)\n",
    "\n",
    "- K-fold iterator variant with non-overlapping groups.\n",
    "- The group are indicated by user.\n",
    "- The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds).\n",
    "- This is use when the predicting power when splitting a group doesn't change (it provides nothing new).\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6. GroupShuffleSplit.\n",
    "\n",
    "- Like `ShuffleSplit` but wrt groups.\n",
    "\n",
    "![alt text](https://i.ibb.co/VpQqKpJ/Screen-Shot-2020-10-01-at-13-16-22.png) \n",
    "\n",
    "---\n",
    "\n",
    "## 1.7. Time-Based Split.\n",
    "\n",
    "![alt text](https://i.ibb.co/YjjXQD9/Screen-Shot-2020-10-01-at-13-32-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Do StandardScaler fit with train set and transform train and test set as needed.\n",
    "    - Test set is unknown, we shouldn't scaled data (fit) on the test set.\n",
    "    \n",
    "---\n",
    "\n",
    "# 2. Bias and Variance.\n",
    "\n",
    "- $x$-axis: Modelcomplexity.\n",
    "- $y$-axis: Error.\n",
    "\n",
    "![alt text](https://i.ibb.co/w6Xs3Wy/Screen-Shot-2020-10-01-at-17-22-46.png)\n",
    "\n",
    "- **Bias:** This part of the generalization error is due to wrong assumptions, such as assuming that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.\n",
    "    - **When the model poorly generalizes.**\n",
    "- **Variance:** This part is due to the modelâ€™s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance, and thus to overfit the training data.\n",
    "\n",
    "![alt text](https://i.ibb.co/2v005H2/Screen-Shot-2020-10-01-at-18-24-44.png)\n",
    "\n",
    "- $\\text{(Low Bias, Low Variance)}$: this is what we strive for.\n",
    "- $\\text{(High Bias, Low Variance)}$: when the prediction are more or less homogeneous (under similar features) but the predictions are wrong.\n",
    "- $\\text{(Low Bias, High Variance)}$: The model is able to more or less predict correctly, but it's sensitive to small changes in the data (small changes give totally different predictions).\n",
    "- $\\text{(High Bias, High Variance)}$: Thw worst case-scenario. Not only the model makes bad prediction but it's also sensitive to data changes.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1. Bias-Variance Decomposition.\n",
    "\n",
    "- Here we consider the MSE loss function.\n",
    "\n",
    "![alt text](https://i.ibb.co/XDRzdhZ/Screen-Shot-2020-10-01-at-18-35-49.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
