{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recap Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://i.ibb.co/DWFZJj9/Screen-Shot-2020-09-29-at-16-25-40.png)\n",
    "\n",
    "- This chosen empirical risk is not good: It can't be optimize (minimize). We can use a upper bound approximation to it, like Hinge Loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2. Hinge Loss.\n",
    "\n",
    "- Hinge Loss is the RHS of the inequality below:\n",
    "\n",
    "![alt text](https://i.ibb.co/yfGtYRM/Screen-Shot-2020-09-29-at-16-28-08.png)\n",
    "\n",
    "\n",
    "Under the assumption that classes are linearly separable (there exists a hyperplane that divides them), we can come up with different hyperplanes to separate them. Which of these hyperplanes is the best one?:\n",
    "\n",
    "![alt text](https://i.ibb.co/qWDNQRt/Screen-Shot-2020-09-29-at-16-30-52.png)\n",
    "\n",
    "In the example above, the best hyperplane is $L2$, since it's located as far away as it can from the classes. In other words, this line not only separates the two classes but also stays as far away from the closest training instances as possible, With this we ensure that new added points will be correctly classified.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. SVM.\n",
    "\n",
    "Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances are called the support vectors (see belowe for details).\n",
    "\n",
    "Нам нужно, чтобы полоса между классами была как можно шире. То есть, хотим максимизировать ширину разделяющей полосы. For this we:\n",
    "\n",
    "1. Normalize the margin such that the minimum of them = 1.\n",
    "2. 1 is the wide set to the streets.\n",
    "\n",
    "![alt text](https://i.ibb.co/jW64QGb/Screen-Shot-2020-09-29-at-16-44-54.png\n",
    "\n",
    "We have change a bit our classification goal (finding hyperplane \\to finding hyperplane with widest streets). Therefore we have to change the target as well.\n",
    "\n",
    "![alt text](https://i.ibb.co/JQ3tcjK/Screen-Shot-2020-09-29-at-16-53-01.png)\n",
    "\n",
    "\n",
    "- We notice that with $$\\max_w\\left\\{ \\frac{(x_{+} - x_{-})}{||w||}\\right\\}$$ are trying to find that vector $w$ (i.e. that hyperplane, such that the projection of $x_{+} - x_{-}$ onto $w$ (i.e. the distance between two points from different classes) is maximized.\n",
    "\n",
    "Therefore the problem statement SVM classification is:\n",
    "\n",
    "- $M_i \\geq 1 \\implies$ every observations is correctly classified.\n",
    "\n",
    "![alt text](https://i.ibb.co/hdBPjHL/Screen-Shot-2020-09-29-at-16-59-07.png)\n",
    "![alt text](https://i.ibb.co/4N1kjBX/Screen-Shot-2020-09-29-at-16-59-24.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 3. What happens if the data is not separable?\n",
    "\n",
    "The condition $M_i \\geq 1 $ is too strong. In real life data set classes are not completely separable. Therefore we introduce some heuristic:\n",
    "\n",
    "![alt text](https://i.ibb.co/ws54sYv/Screen-Shot-2020-09-29-at-17-02-36.png)\n",
    "\n",
    "This means that we allow some misclassifications.\n",
    "\n",
    "![alt text](https://i.ibb.co/Ntmfp35/Screen-Shot-2020-09-29-at-17-03-00.png)\n",
    "\n",
    "We see that the optimization task for nonlinear-separable data turns out to be the same as the optimization task for linear-separable data with Hinge Loss.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Regularization in SVM.\n",
    "\n",
    "![alt text](https://i.ibb.co/0yb4CVp/Screen-Shot-2020-09-29-at-17-07-47.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Solving the Optimization Task for SVM.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1. KKT Refresher.\n",
    "\n",
    "\n",
    "![alt text](https://i.ibb.co/c18WwkR/Screen-Shot-2020-09-29-at-17-18-38.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2. KKT for the SVM Task.\n",
    "\n",
    "![alt text](https://i.ibb.co/nzvvGhj/Screen-Shot-2020-09-29-at-17-19-24.png)\n",
    "\n",
    "## $$\\Downarrow$$\n",
    "\n",
    "![alt text](https://i.ibb.co/xDg4zYc/Screen-Shot-2020-09-29-at-17-20-11.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3. From KKT We Get:\n",
    "\n",
    "![alt text](https://i.ibb.co/zVmYwKN/Screen-Shot-2020-09-29-at-17-35-34.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 5.4. Dual Task for SVM.\n",
    "\n",
    "![alt text](https://i.ibb.co/ZJk3Zxd/Screen-Shot-2020-09-29-at-17-39-41.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5. SVM: General Solution.\n",
    "\n",
    "![alt text](https://i.ibb.co/NCpDDtt/Screen-Shot-2020-09-29-at-17-43-24.png)\n",
    "\n",
    "- (x, x_i) can be interpreted as a similarity measure. Then we can use any similarity measure we like. This brings us to Nonlinear SVM.\n",
    "    - More formally, we can use another function such that can be represented as a dot product in another Hilbert Space.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Nonlinear SVM.\n",
    "\n",
    "![alt text](https://i.ibb.co/FWYBnj4/Screen-Shot-2020-09-29-at-17-54-07.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1. Examples.\n",
    "\n",
    "![alt text](https://i.ibb.co/6N6pd6z/Screen-Shot-2020-09-29-at-17-57-58.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 1. PCA.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1. SVD.\n",
    "\n",
    "![alt text](https://i.ibb.co/Wf2ZYGQ/Screen-Shot-2020-09-29-at-18-05-17.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2. Dimensionality Reduction.\n",
    "\n",
    "![alt text](https://i.ibb.co/FK2N7sV/Screen-Shot-2020-09-29-at-18-06-21.png)\n",
    "\n",
    "![alt text](https://i.ibb.co/LCW3pZD/Screen-Shot-2020-09-29-at-18-08-09.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3. Dimensionality Reduction with SVD.\n",
    "\n",
    "![alt text](https://i.ibb.co/MB3c6K0/Screen-Shot-2020-09-29-at-18-10-00.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4. Theorem Eckart-Young.\n",
    "\n",
    "![alt text](https://i.ibb.co/yqF9ZDj/Screen-Shot-2020-09-29-at-18-10-56.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5. Essence of PCA.\n",
    "\n",
    "![alt text](https://i.ibb.co/n1Mqj9b/Screen-Shot-2020-09-29-at-18-13-37.png)\n",
    "\n",
    "- Consider the columns of matrix $V$ as basis vectors.\n",
    "- Then, the $i$-th column of matrix $U\\Sigma$ is the projection of $A$ onto the $i$-th basis vector of $V$, since $AV = U\\Sigma$.\n",
    "- The columns of $V$ are called the principanl directions of $A$ and the columns of $U\\Sigma$ the principal components of $A$.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6. PCA: Effective Dimensionality.\n",
    "\n",
    "**<font color=red>All of this is shitty explained</font>**\n",
    "\n",
    "![alt text](https://i.ibb.co/crs22hK/Screen-Shot-2020-09-29-at-18-18-31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. PCA in Practice.\n",
    "\n",
    "![alt text](https://i.ibb.co/ZKVFCfr/Screen-Shot-2020-09-29-at-18-43-33.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 3. PCA: My Two Cents.\n",
    "\n",
    "- PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "- $X \\in \\mathbb{R}^{n\\times p}$.\n",
    "- $W \\in \\mathbb{R}^{p\\times p}$.\n",
    "    - The columns of $W$ are the eigenvectors of $X^{\\top}X$.\n",
    "    - Since $X$ is considered column-wise centered, $X^{\\top}X$ is proportional to the covariance matrix of $X^{\\top}$.\n",
    "- **Principal Component Decomposition:** $T = XW$ - the score matrix.\n",
    "- **SVD:** $X = U\\Sigma W^{\\top}$\n",
    "    - $U \\in \\mathbb{R}^{n\\times n}$ - matrix of left singular vectors.\n",
    "    - $\\Sigma \\in \\mathbb{R}^{n\\times p}$ - matrix of singular values $\\sigma_i$\n",
    "    - $W \\in \\mathbb{R}^{p\\times p}$ - matrix of right singular vectors.\n",
    "- $\\implies X^{\\top}X = W(\\Sigma\\Sigma^{\\top})W^{\\top} \\implies (X^{\\top}X)W = W\\hat{\\Sigma}^2 \\implies $\n",
    "    - The columns of $W$ are the eigenvectors of $X^{\\top}X$.\n",
    "    - $\\sigma_i^2$ are its eigenvalues.\n",
    "- $\\implies T = XW = U\\Sigma W^{\\top}W = U\\Sigma \\implies$\n",
    "    - The columns of $T$ are the left singular eigenvector of $X$ multiplied by the corresponding eigenvalues.\n",
    "- **Truncated matrix:** $T_L = T = XW_L = U_L\\Sigma_L \\in \\mathbb{R}^{n\\times L}$ is obtained by considering only the first $L$ largest singular values and their singular vectors\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
