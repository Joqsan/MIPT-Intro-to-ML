{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remember:** Модель - это отображение из пространствва признаков в пространство таргетов.\n",
    "- NN доминируют в задачах с структурированными данными, то есть в данных наблюдается какая структура. Например, \n",
    "    - Изображения (есть порядок между пикселями).\n",
    "    - Обработка истетственного языка (есть порядок между словами).\n",
    "    - Текст (есть порядок)\n",
    "    \n",
    "    \n",
    "- Example of non-structural data: Titanic. In this case mat be better and faster to train a RF or Gradient Boosting instead of NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Network Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://cs231n.github.io/assets/nn1/neural_net2.jpeg)\n",
    "\n",
    "All connection strengths for a layer can be stored in a single matrix. For example, the first hidden layer’s weights `W1` would be of size `[4x3]`, and the biases for all units would be in the vector `b1`, of size `[4x1]`. Here, every single neuron has its weights in a row of `W1`, so the matrix vector multiplication `np.dot(W1,x)` evaluates the activations of all neurons in that layer. Similarly, `W2` would be a `[4x4]` matrix that stores the connections of the second hidden layer, and `W3` a `[1x4]` matrix for the last (output) layer.\n",
    "\n",
    "For **each neuron** in the $i$-th fully connected layer we have that its input size is $\\text{n_input}_i$ and its output size is $1$ (because of the activation function).\n",
    "\n",
    "But we usually forward passing is done through the whole $i$-th layer using matrix-vector operations.\n",
    "\n",
    "\n",
    "$\\implies$ that for the **$i$-th layer as a whole**, its input size is $\\text{n_input}_i$ and output size is $\\text{n_ouput}_i$, where $\\text{n_output}_i =$ number of neurons in the $i$-th layer.\n",
    "\n",
    "\n",
    "- That is, for a given $i$-th layer $$\\text{W}_i\\text{.shape} = (\\text{n_output}_i, \\text{n_input}_i) = (\\text{n_input}_{i+1}, \\text{n_output}_{i-1})$$.\n",
    "    - Of course, for a fully connected layer,each neuron gets the same number of inputs.\n",
    "\n",
    "\n",
    "- The original data set $X = X^{(0)}$ (the input layer is not counted) has shape $(\\text{n_instances}, \\text{n_features}) = (\\text{n_instances}, \\text{n_input}_0)$.\n",
    "\n",
    "\n",
    "- If $X^{(i-1)}$ is the dataset transformed after passing through the $(i-1)$-th layer, then $$\\text{X}^{(i-1)}\\text{.shape} = (\\text{n_instances}, \\text{n_input}_i) = (\\text{n_instances}, \\text{n_output}_{i-1})$$\n",
    "\n",
    "\n",
    "- Therefore the transformation at the $i$-th layer has to be of the form $$X^{(i)} = X^{(i-1)}W_i^{\\top} + b_i$$ where $$X^{(i-1)}W_i^{\\top}\\text{.shape} = (\\text{n_instances}, \\text{n_input}_i)(\\text{n_input}_i, \\text{n_output}_i)$$\n",
    "\n",
    "\n",
    "- $b_{ij}$ - bias term at the $j$-th neuron in the $i$-th layer. It's just a number.$ \\implies b_i = (\\text{number of neurons in the }i\\text{-th layer}, 1) = (\\text{n_ouput}_i, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Until now.\n",
    "\n",
    "- Until know we are the ones who have to come up with feature engineering and feature extraction:\n",
    "\n",
    "![alt text](https://i.ibb.co/CzWWhkk/Screen-Shot-2020-11-02-at-18-24-14.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 2. What we want.\n",
    "\n",
    "- But we'd want that feature extraction could have done automatically. This means that feature extraction has to be parameterized:\n",
    "\n",
    "![alt text](https://i.ibb.co/KWqvZQN/Screen-Shot-2020-11-02-at-18-26-39.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 3. How to parameterized the feature extraction?\n",
    "- This can be done by using a linear model + sigmoid function:\n",
    "    ![alt text](https://i.ibb.co/tJ9Q4vC/Screen-Shot-2020-11-02-at-18-28-23.png)\n",
    "\n",
    "- That is, we make a nonlinear transformation between the first and second model.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. How can be perform a nonlinear transformation?\n",
    "\n",
    "- For this are used so-called activation functions.\n",
    "\n",
    "![alt text](https://i.ibb.co/PNWh3Tg/Screen-Shot-2020-11-02-at-18-33-07.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Notation.\n",
    "\n",
    "- NN - это последовательность преобразований, которая из исходного признакового пространства преобразует объект какое-то целевое а потом в метку классов.\n",
    "\n",
    "\n",
    "- Layer – a building block for NNs :\n",
    "    - Dense/Linear/FC layer: $f(x) = Wx+b$.\n",
    "    - Nonlinearity layer: $f(x) = \\sigma(x)$.\n",
    "    - Input layer - представление данных в исходных признаках.\n",
    "    - Оutput layer - представление данных в целевом виде. Например выходных слой порождает метку классов.\n",
    "    - A few more we will cover later.\n",
    "\n",
    "\n",
    "- Activation function – function applied to layer output\n",
    "    - sigmoid.\n",
    "    - $\\tanh$.\n",
    "    - $\\mathrm{ReLU}$.\n",
    "    - Any other function to get nonlinear intermediate signal in NN.\n",
    "\n",
    "\n",
    "- Backpropagation – a fancy word for the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Backpropagation.\n",
    "\n",
    "- Каждый последующий слой получен каким-то преобразованием над выходом предыдущих слоев. То есть между слоями у нас есть функция преобразования, которая зависит от каких-то параметров $\\implies$ есть шанс, что такие параметры сможем их выучить. \n",
    "- Чтобы их выучить мы воспользуемся методом обратного распространения ошибки.\n",
    "- Позволяет нас шаг ха шагом считать градинеты для каждого из слоев нашей сети.\n",
    "\n",
    "![alt text](https://i.ibb.co/gVwftcR/Screen-Shot-2020-11-02-at-23-26-08.png)\n",
    "![alt text](https://i.ibb.co/bPSm7Yc/Screen-Shot-2020-11-02-at-22-08-50.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Backpropagation in the case of linear regression:\n",
    "\n",
    "![alt text](https://i.ibb.co/fDhFR2r/Screen-Shot-2020-11-02-at-23-31-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 7. Backpropagation Example 1.\n",
    "\n",
    "![alt text](https://i.ibb.co/BPZx2dH/Screen-Shot-2020-11-09-at-22-53-37.png)\n",
    "![alt text](https://i.ibb.co/bbH5NgJ/Screen-Shot-2020-11-09-at-22-53-53.png)\n",
    "\n",
    "![alt text](https://i.ibb.co/X3p6v8h/Screen-Shot-2020-11-02-at-23-45-30.png)\n",
    "![alt text](https://i.ibb.co/9rn8t4x/Screen-Shot-2020-11-02-at-23-46-41.png)\n",
    "\n",
    "- Why does it matter that we known the numeric values of the gradients?\n",
    "    - Because since our function is differentiable, we know the analytic solution of each of the performed transformations.\n",
    "    \n",
    "    \n",
    "- What happens if $x$, $y$ and $z$ are vectors?\n",
    "    - We do the same thing.\n",
    "    \n",
    "---\n",
    "\n",
    "\n",
    "# 8. Backpropagation Example 2.\n",
    "\n",
    "![alt text](https://i.ibb.co/8Psr8qy/Screen-Shot-2020-11-09-at-22-34-59.png)\n",
    "\n",
    "- Backward red arrows: the gradient of passed-by node.\n",
    "- Let at the $k$-th node perform the operation $f_k$, giving the value $v_k$. Then by construction $f_k(v_{k-1}) = v_k$.\n",
    "- Let $d_j$ be the derivative value after node $j$ (traversing from left to right).\n",
    "    - If $n$ is the last node, then $d_n = \\frac{df}{df} = 1$.\n",
    "- Backpropagation from node $i$ to node $i-1$ (for $i = [n,n-1, n-2,\\ldots, 1]$):\n",
    "    - We know the derivative value after node $i$: $d_i$.\n",
    "    - Backward red arrow passing by node $i$ is just $f'_i$.\n",
    "    - Calculate $f'_i(v_{i-1})$.\n",
    "    - Then $d_{i-1} = f'_i(v_{i-1})\\cdot d_i$.\n",
    "    \n",
    "    - **Example:**\n",
    "        - $d_n = 1$.\n",
    "        - $f'_n = -\\frac{1}{x^2}$.\n",
    "        - $v_{n-1} = 1.37$.\n",
    "        - $f'_n(v_{n-1}) = -\\frac{1}{1.37^2} = -0.53$.\n",
    "        - $d_{n-1} = f'_n(v_{n-1})\\cdot d_n = -0.53 \\cdot 1 = -0.53$.\n",
    "        \n",
    "    - **Notes:**\n",
    "        - If node $i$ is $\\cdot (-1) \\implies$ node $i$ is $-x \\implies f_i = -1$.\n",
    "        - If node $i$ is $+ \\implies$ node $i$ is $x + a \\implies f_i = 1$.\n",
    "        - If node $i$ is $* \\implies$ node $i$ is $x\\cdot a \\implies f_i = a$.\n",
    "            - $*$ is a binary operation, i.e. $\\text{left} * \\text{right}$ so:\n",
    "                - $d_{\\text{left}} = f'_i(v_{\\text{left}})\\cdot d_i = v_{\\text{right}}\\cdot d_i$.\n",
    "                - $d_{\\text{right}} = f'_i(v_{\\text{right}})\\cdot d_i = v_{\\text{left}}\\cdot d_i$.\n",
    "                - We swap the values (green arrows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://i.ibb.co/DWM0kQq/Screen-Shot-2020-11-03-at-01-25-24.png)\n",
    "![alt text](https://i.ibb.co/yfVsPsS/Screen-Shot-2020-11-03-at-01-25-30.png)\n",
    "\n",
    "### <font color=green>WHY DOING BACKPROPAGATION IS USEFUL?</font>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 9. Why do we need some sequential transformation?\n",
    "\n",
    "Поскольку \n",
    "\n",
    "1) Каждое предыдующее преобразование порождает более сложное признаковое пространство, так как мы взяли исходное и нелинейным образом мы его преобразовали.\n",
    "\n",
    "2) Если мы подбираем параметры этих преобразования каким-то хорошим образом, то мы сможем себе найти более информативное признаковое пространство.\n",
    "\n",
    "---\n",
    "\n",
    "# 10. Activation Functions: Recap.\n",
    "\n",
    "- Функции активации применяются после линейного или квазилинейного преобразования над исходными данными.\n",
    "- Функция активации дает нам нелинейность. **<font color=red>Why don't we parameterize them?</font>**\n",
    "\n",
    "---\n",
    "\n",
    "## 10.1. Sigmoid Function.\n",
    "\n",
    "![alt text](https://i.ibb.co/f8x4Lqg/Screen-Shot-2020-11-03-at-01-44-45.png)\n",
    "\n",
    "\n",
    "**Other Cons:**\n",
    "\n",
    "1) It kills the gradients in the sense that if we get as too positive or two negative values of $z$, then $\\sigma(z)$ tends to $1$ or $0 \\implies \\sigma'(z) \\to 0$.\n",
    "\n",
    "2) NN are basically linear models with nonlinear activation between layers. As such are sensitive to feature scaling and it's recommended to normalize the data before training, that is, that the data were centralized. Since sigmoid is a nonnegative function, it tends to shift the mean towards positive values.\n",
    "\n",
    "---\n",
    "\n",
    "## 10.2. $\\tanh$.\n",
    "\n",
    "![alt text](https://i.ibb.co/PxBz9BG/Screen-Shot-2020-11-03-at-01-59-03.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 10.3. ReLU.\n",
    "\n",
    "![alt text](https://i.ibb.co/tcZNKW0/Screen-Shot-2020-11-03-at-02-00-53.png)\n",
    "\n",
    "\n",
    "**Other Cons:**\n",
    "\n",
    "- For negative values of the argument it kills the gradient, since the gradient for $z \\leq 0$ is $0$.\n",
    "\n",
    "---\n",
    "\n",
    "## 10.4. Leaky ReLU.\n",
    "\n",
    "![alt text](https://i.ibb.co/m5f538N/Screen-Shot-2020-11-03-at-02-04-36.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 10.5. Parametric ReLU.\n",
    "\n",
    "![alt text](https://i.ibb.co/z54SwMD/Screen-Shot-2020-11-03-at-02-05-44.png)\n",
    "\n",
    "- $\\alpha$ is a hyperparameter.\n",
    "- It's not that often used.\n",
    "\n",
    "---\n",
    "\n",
    "## 10.6. Exponential Linear Units (ELU).\n",
    "\n",
    "![alt text](https://i.ibb.co/HBJHhbJ/Screen-Shot-2020-11-03-at-02-07-00.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 11. Activation Functions: Conclusions.\n",
    "\n",
    "- Use ReLU as baseline approach.\n",
    "- Be careful with the learning rates.\n",
    "- Try out Leaky ReLU or ELU.\n",
    "- Try out tanh but do not expect much from it.\n",
    "- Do not use Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
