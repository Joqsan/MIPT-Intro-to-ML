{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# 1. Warm-Up Test Review.\n",
    "\n",
    "1) Imagine you've added decision tree on the top (to the end) of the neural network. What will happen?\n",
    "\n",
    "Answer: \n",
    "\n",
    "- This is an unusual model, and neither the NN as a whole nor the DT at the end will bring some meaningful result:\n",
    "    - We will be unable to train the model using gradient methods, since trees are non-differentiable, which prevent us from training the network by backpropagation $\\implies$ неуронка станет необучаемой.\n",
    "    - Дерево тоже станет необучаемой, потому что неуронка при этом является иницилицированны случайным образом преоразованием, и поэтому дерево будет обучаться на рамдомных признаках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Why tanh could be preferred to sigmoid as activation function?\n",
    "\n",
    "Answer: \n",
    "\n",
    "- It's (tanh's) output value is centred, the sigmoid's is not.\n",
    "- tanh output can be interpreted as probability, while sigmoid is just some number.\n",
    "    - **False:** The function takes negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) What are the \"pros\" of the ReLU activation function?\n",
    "\n",
    "Answer: \n",
    "\n",
    "- Easy to compute the derivative (-> gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Neural networks can be represented as combination of\n",
    "\n",
    "Answer:\n",
    "\n",
    "- Linear functions and nonlinear activations.\n",
    "- Black boxes with some magic inside.\n",
    "\n",
    "5) Blending of logistic regressions can be represented as:\n",
    "\n",
    "Answer:\n",
    "\n",
    "- It can be represented as a NN with two layers: the first layer being the one with the logistic regression model, and the second one with the meta learner.\n",
    "\n",
    "6) Backpropagation through the ReLU:\n",
    "\n",
    "- Requires computation of the exact gradient of the function.\n",
    "- Is equivalent to multiplying the gradient with boolean mask.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
