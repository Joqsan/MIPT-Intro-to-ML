{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Warm Up Test.\n",
    "\n",
    "1) Batch normalization layer.\n",
    "\n",
    "- Requires batch input on inference (test) phase to get the `mean` and `std` values.\n",
    "    - **NO.**\n",
    "- Can work even on one element input (when trained).\n",
    "    - **YES.**\n",
    "- Compute `mean` and `std` over all the `neurons` in the previous layer.\n",
    "    - **NO.**\n",
    "- Compute `mean` and `std` for every `neuron` independently over the batch.\n",
    "    - **YES.**\n",
    "\n",
    "2) Batch normalization:\n",
    "\n",
    "- Makes the fitting (training) process more stable so the learning rate can be increased. This is is so because, batch normalization prevents gradient explosion on subsequent layers and, consequently, allows to increase the learning rate (it prevent (co)variance shift from happening).\n",
    "\n",
    "3) Dropout:\n",
    "\n",
    "- Increases the network quality on the inference stage.\n",
    "    - **NOT TRUE.** It's the other way around. For example if our dropout probability is $0.5$, then our model won't be able to generalize complex models well as the whole network would.\n",
    "    - The answer is yes. **<font color=red>Why?</font>** Because the model is less prone to overfitting.\n",
    "- Makes the network more stable on noisy data.\n",
    "    - Yes, because at some point the noisy features will be dropout, and the model will train without them.\n",
    "- Is similar to ensembling methods in some way.\n",
    "    - Because each time we perform dropout we work with some part of the network. Each time we dropout a random subset of neurons, so we can assume that the subnetworks we work with are different.\n",
    "- Fixes the vanishing gradient problem.\n",
    "    - There is no link between dropour and vanishing gradient.\n",
    "- Requires scaling factor on the inference stage.\n",
    "    - Yes (see **Lecture10** for details).\n",
    "    \n",
    "4) Momentum update to SGD:\n",
    "\n",
    "- Allows to avoid some of the local minimas.\n",
    "    - Yes.\n",
    "- Makes the network less prone to overfitting.\n",
    "    - There is no connection between overfitting and SGD.\n",
    "- Regularizes the network decision to make the solution more stable.\n",
    "    - Probably yes, since it regularizes the path SGD can take when searching for the minimum.\n",
    "- Is named after its inventor.\n",
    "    - xD.\n",
    "- Speeds up the convergence in some cases.\n",
    "    - Yes, **in some cases**. There are some cases where momentum can converge slower than plain SGD (the same goes for Nesterov's). In both cases using Adam (normalizing the gradient) can be of help.\n",
    "    \n",
    "5) RMSprop:\n",
    "\n",
    "- Computes the squared norm of gradient to speed up the convergence and avoid local minima.\n",
    "    - Yes.\n",
    "- Computes the squared norm of gradient to stabilize the gradient over different dimensions.\n",
    "    - Yes.\n",
    "- Is incompatible with the momentum (or Nesterov momentum) SGD upgrade.\n",
    "    - False. See Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
