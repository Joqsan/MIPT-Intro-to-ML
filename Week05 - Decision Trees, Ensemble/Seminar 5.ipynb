{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 0. Warm-Up Test.\n",
    "\n",
    "--- \n",
    "\n",
    "## Q1.\n",
    "\n",
    "- Decision Trees are not differentiable models, so first or second order gradient descent method don't work with them.\n",
    "- It can be thought of as if they were constructed by a greedy optimization method: the feature space is finite and the splits are defined by exhausting all the pairs (feature, threshold) and by selecting the one that gives the best split according to some information criterion.\n",
    "\n",
    "---\n",
    "\n",
    "## Q2.\n",
    "\n",
    "- Decision Trees are prone to overfitting. Bagging helps to deal with this by reducing the overall error (see lecture).\n",
    "- With Bagging the predictors tends to be less correlated.\n",
    "\n",
    "---\n",
    "\n",
    "## Q4.\n",
    "\n",
    "- Decision Tree are not good in extrapolating data. Decision Tree are good in making predictions within the subspace we got after splitting the feature space, but not outside.\n",
    "    - The feature space is finite, and the portion of the feature space \"occupied\" by the dataset observations is also finite. If we try to predict for some observation outside the region of space occupied by the observation, then either the Decision Tree won't be able to make a good prediction or it will be quite poor (for instance $x^{(j)} \\in [-5, 5]\\ \\forall$ instances, and $b(x^{(j)}_i) = \\text{blue}$ for all $x^{(j)}_i < -3$ but we added a new instance $x^{(j)}_u = -10^7)$. The prediction will be $\\text{blue}$ but that would be a very rough approximation of the real target for $x^{(j)}_u$.\n",
    "    - More formally, Decision Tree are not at predicting on observations that lie outside of the convex hull of the features vectors: $\\mathrm{Conv}\\{x^{(1)}, x^{(2)}, \\ldots, x^{(m)}\\}$.\n",
    "    \n",
    "---\n",
    "\n",
    "## Q7.\n",
    "\n",
    "- Decision Trees can be used for Anomaly (outlier) Detection by simply limiting the depth of tree. The outliers will be out of the range used for making the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
