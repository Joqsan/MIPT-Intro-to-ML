{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision Tree.\n",
    "\n",
    "- A decision tree is just a tree, that at each step divides our data set (выборку) into two subsets (подвыборки) используя какой-то предикат, обычно используя одну фичу с каким-то пороговым значением.\n",
    "\n",
    "- In the plot below:\n",
    "    - $y$-axis: petal length.\n",
    "    - $x$-axis: petal width.\n",
    "    \n",
    "![alt text](https://i.ibb.co/4dVCD35/Screen-Shot-2020-10-05-at-17-15-39.png)\n",
    "\n",
    "As we see above some points are incorrectly classified. To fix this we can make the tree even deeper, but we have to be careful, otherwise we end up overfitting (переобучение) the data.\n",
    "\n",
    "- For a classification task, a Decision Tree **predicts a constant** for every subspace we got after splitting.\n",
    "- For regression we have: ![alt text](https://i.ibb.co/Qdcb3Rp/Screen-Shot-2020-10-05-at-17-24-21.png) Here we also notice that the tree also predict some constant for each subpace we got after splitting (in this case the data is one-dimensional and each subpace is some subinterval).\n",
    "    - Each constant is given by the each tree leaf.\n",
    "\n",
    "In other words: A decision tree is a piecewise constant function that divides our feature space into subspaces and each suspace is given a constant as predicted value.\n",
    "- For classification the predicted value is a class label.\n",
    "- For regression is just a number.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. How to construct a Decision Tree.\n",
    "\n",
    "![alt text](https://i.ibb.co/dtd7XtG/Screen-Shot-2020-10-05-at-17-30-27.png)\n",
    "\n",
    "- Since a tree can be viewed as a piecewise constant function, it follows that gradient methods for its optimization won't work.\n",
    "- Workaround: apply greedy optimization.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. How to split data properly.\n",
    "\n",
    "\n",
    "- Пусть мы перебираем всевозможные трешхолды и всевозможные признаки. Then what criteria to work with for determining if the split was good or not?\n",
    "- Let $Q$ be our data set (выборка).\n",
    "- Then we choose some feature $j$ and threshold $t$ to make the split.\n",
    "- To understand how good this split is let's introduce\n",
    "\n",
    "![alt text](https://i.ibb.co/hsQNywZ/Screen-Shot-2020-10-05-at-17-41-56.png)\n",
    "\n",
    "- That is some weighted sum of some function $H(\\cdot)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. How to choose $H$.\n",
    "\n",
    "![alt text](https://i.ibb.co/0fYYJNm/Screen-Shot-2020-10-05-at-17-44-42.png)\n",
    "\n",
    "- We'd want to make the split such that in one of the leaves (leftmost or rightmost) we end up getting observations from just one of the classes.\n",
    "- This is the same as saying that we'd want to make the split, such that the observations in one of the the leaves is as homogeneous as possible.\n",
    "    - We'd want the data to be as ordered as possible, with instances from one class on the left side and from the other on the right side.\n",
    "\n",
    "\n",
    "- $H(R)$ measures heterogeneity, so it has to measure the amount of missclasification. Therefore we want $H(R)$, and in turn $S(R)$, to be as small as possible.\n",
    "\n",
    "![alt text](https://i.ibb.co/54rpJyG/Screen-Shot-2020-10-05-at-18-01-16.png)\n",
    "\n",
    "- Misclassification criteria = Classification error. \n",
    "    - It's simply the fraction of training observations in a region that do not belong to the most common class.\n",
    "    - Or the probability of misclassifying if we predict the most common class.\n",
    "    - This is not a good approach, because it ignores all non-common classes, which is not good for multiclass classification.\n",
    "    \n",
    "Multiclass Case:\n",
    "\n",
    "![alt text](https://i.ibb.co/QMbpKqd/Screen-Shot-2020-10-05-at-18-24-22.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1. Entropy.\n",
    "\n",
    "- $M$ is a constant (arbitrary).\n",
    "- $K$ - the number of classes.\n",
    "- Entropy is somewhat similar to logarithm of likelihood estimation.\n",
    "- In ML a set's entropy is zero if it contains instances of only one class (for sure, the amoun of instance per class is $1 = N/n_i$).\n",
    "- $p_k \\in [0,1]$.\n",
    "- $\\sum\\limits_{k=0}^K p_k = 1$.\n",
    "\n",
    "![alt text](https://i.ibb.co/pQztXXp/Screen-Shot-2020-10-05-at-18-26-00.png)\n",
    "\n",
    "**Intuition:**\n",
    "\n",
    "![alt text](https://i.ibb.co/pwKzKgm/Screen-Shot-2020-10-05-at-19-11-19.png)\n",
    "\n",
    "**Intuition (my two cents):**\n",
    "\n",
    "- $p_k$ is the frequency with which the $k$-th class appears in the $m$-th split.\n",
    "- $1/p_k = T_k$ is the period, i.e. is the number of instances after which we expect (on average) to **have encountered exactly one** instance of the $k$-th class.\n",
    "    - It's a kind of summary of the whole information about the instances in the $m$-th split. In the $m$-th split, on average, we have $T_k - 1$ instead not of class $k$ and $1$ instance from that class.\n",
    "- $\\log(1/p_k)$ - is the number of (nat)bits we need to encode that information summary.\n",
    "    - We notice that if $p_k = 1 \\implies 1/p_k =1 \\implies \\log(1/p_k) = 0$ and we need to encode nothing at all. This means that we talk about _storing (encoding, receiving) information_ only when we can't capture all the split information just by looking and the $k$-th class.\n",
    "- $\\implies S = \\mathbb{E}\\{\\text{number of bits for encoding the whole information}\\}$.\n",
    "\n",
    "\n",
    "- $S \\in [0, 1/k]$. Let's find the maximum value $S$ can attain: ![alt text](https://i.ibb.co/9GXCsRy/Screen-Shot-2020-10-05-at-20-54-02.png)\n",
    "    - The more classes we have, the larger is the entropy.\n",
    "    - The entropy (chaos, disorder) is maximum, when the classes have uniform discrete distribution.\n",
    "    - **<font color=red>We want to minimize entropy of each split.</font> If we do this, then we can say that our Decision Tree _is good_.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. How predict with Decision Trees.\n",
    "\n",
    "- We now know how to construct a Decision Tree and how to make splits, but once we reach a leaf, what class should we predict for those instances?\n",
    "    - Suppose we are is some leaf, where there are instances from different classes.\n",
    "    - Let calculate (Negative) Log Likelihood.\n",
    "    - $C_k$ is the quantity we want to find. ![alt text](https://i.ibb.co/T0wm5sS/Screen-Shot-2020-10-05-at-21-08-08.png) $\\implies C_k = p_k$. This means that in each leaf we have to predict the выборочную вероятность for each class.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2. Gini Impurity.\n",
    "\n",
    "![alt text](https://i.ibb.co/NpLGqKy/Screen-Shot-2020-10-05-at-21-28-12.png)\n",
    "\n",
    "**Intuition (my two cents, from 3 years ago):**\n",
    "\n",
    "- Gini impurity. It's a measure of misclassification, which applies in a multiclass classifier context.\n",
    "    - Let's have a look at the $i$-th child node.\n",
    "    - Let pick an instance belonging to the $k$-th class (among a total of $n$ classes). There is a probability $p_{i,k}$ of picking an instance from that class.\n",
    "    - $$p_{i,k} = \\frac{\\text{# of instances of class } k \\text{ in the }i\\text{-th node}}{\\text{total # of instances in that node}}$$\n",
    "    - Let suppose that that instance was wrongly classified. There is a $1 - p_{i,k}$ of probability for this to happen.\n",
    "    - Then the probability of picking an instance from one particular class **and** misclassifying is: $g_{i,k} = p_{i,k}(1 - p_{i,k})$\n",
    "    - Performing the procedure describe above for all remaining classes and summing up, we find that:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_i &= \\sum_{k=1}^n g_{i,k}\\\\\n",
    "    &= \\sum_{k=1}^np_{i,k}(1 - p_{i,k})\\\\\n",
    "    &= \\sum_{k=1}^n p_{i,k}  - \\sum_{k=1}^n p_{i,k}^2\\\\\n",
    "    &= 1 - \\sum_{k=1}^n p_{i,k}^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Therefore $G_i = \\mathbb{E}\\{\\text{misclassified instances at node } i\\}$ or $\\mathbb{P}\\{\\text{misclassified instances at node } i\\}$.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3. Information Criteria.\n",
    "\n",
    "![alt text](https://i.ibb.co/D402JrV/Screen-Shot-2020-10-05-at-21-47-18.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Decision Tree Construction Wrap-Up.\n",
    "\n",
    "- Exhaust all possible features and all possible thresholds.\n",
    "    - In practice we don't go through every possible combination of these pair. There some techniques as tricks that prevent us from doing this (dynamic programming and choosing $t$ by discrete steps).\n",
    "- For each pair (feature, threshold) calculate the splitting criteria $G(j, t)$.\n",
    "- We choose the pair giving the least $G(j, t)$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. What about Regression?\n",
    "\n",
    "![alt text](https://i.ibb.co/q1QLxW0/Screen-Shot-2020-10-05-at-23-00-54.png)\n",
    "\n",
    "- A tree at each leaf predicts some constant. Therefore, let the Decision Tree that predicts the optimal constant for MSE: mean.\n",
    "    - If $H(R)$ is MAE, then the optimal constant is the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Pruning.\n",
    "\n",
    "- Usually stopping criterion for a tree depth are:\n",
    "    - depth itself.\n",
    "    - number of childs.\n",
    "    - number of leaves.\n",
    "    \n",
    "\n",
    "![alt text](https://i.ibb.co/k1WVLxb/Screen-Shot-2020-10-05-at-23-20-44.png)\n",
    "\n",
    "- The method mentioned above are pre-pruning.\n",
    "- Post-pruning: take the tree and start cutting some branches. ![alt text](https://i.ibb.co/h2Cn2gY/Screen-Shot-2020-10-05-at-23-22-23.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Decision Trees: Important Aspects.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1. Missing Values.\n",
    "\n",
    "- A tree can deal with missing values.\n",
    "\n",
    "![alt text](https://i.ibb.co/HNNGcNC/Screen-Shot-2020-10-06-at-00-20-14.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 7.2. Decision Trees as Linear Models.\n",
    "\n",
    "![alt text](https://i.ibb.co/MZSPKqG/Screen-Shot-2020-10-06-at-00-24-18.png)\n",
    "\n",
    "- This resembles a linear model:\n",
    "    - $w_j$ are the weights.\n",
    "    - $[x \\in J_j]$ - indicators are the new features.\n",
    "    \n",
    "----\n",
    "\n",
    "## 7.3. What about categorical features.\n",
    "\n",
    "- If $x^{(j)} \\in \\mathbb{R}$ or $\\in \\{0, 1\\}$, then the splitting process can be done as described above.\n",
    "- If $x^{(j)} = \\overline{1, n}$, then instead of take one of the values as threshold and make the split (sending all instances $\\leq$ to the value to the right and the rest to the left). **<font color=red>???</font>**\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Bootstrap.\n",
    "\n",
    "- Bootstrap: we sample with replacement (each predictor in the ensemble we sample the same instance).\n",
    "- $\\equiv$ doing `np.random.randint(0, m)` $m$ times.\n",
    "- Now let say that we repeat the step above $N$ times. In this way we generate $N$ datasets $X_j,\\ j = \\overline{1,N}$.\n",
    "- $b_j(x)$ - prediction of model $j$ for the  $X_j$ dataset.\n",
    "- $y(x)$ - real target.\n",
    "![alt text](https://i.ibb.co/rpVP9vj/Screen-Shot-2020-10-06-at-00-38-42.png)\n",
    "\n",
    "Now suppose that:\n",
    "\n",
    "![alt text](https://i.ibb.co/m4WrVZD/Screen-Shot-2020-10-06-at-00-50-11.png)\n",
    "\n",
    "BUT:\n",
    "\n",
    "![alt text](https://i.ibb.co/pZH0SWx/Screen-Shot-2020-10-06-at-00-53-07.png)\n",
    "\n",
    "- Let the error be biased. But we also wanted the errors be uncorrelated (that is, the model are not strongly similar to each other). What to do? **BAGGING**.\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Bagging.\n",
    "\n",
    "- Decreases the variance if the basic algorithms are not correlated.\n",
    "\n",
    "---\n",
    "\n",
    "# 10. Random Forest.\n",
    "\n",
    "- In order to make each model more uncorrelated to each other each tree just use a random subset of the original features. This is called **Random Subpace Method**.\n",
    "- $$\\text{Bagging} + \\text{RMS} = \\text{random sampling with replacement} + \\text{random feature sampling} = \\text{Random Forest}$$\n",
    "- OOB (Out-Of-Bag Estimation).\n",
    "    - When doing Bagging, even though we perform sampling with replacement, there are observations $\\{x_{s}\\}$ that weren't picked to form the sub-datasets $\\{X_{j_k}\\} \\subset \\{X_j\\}_1^m$.\n",
    "    - Therefore, since $\\{x_{s}\\}$ weren't seen by $\\{X_{j_k}\\}$, once trained their respective model we can use the set of unseen observations $\\{x_{s}\\}$ for validation.\n",
    "    - This means we can get some upper bound estimation for the error without even using the validation set.\n",
    "        - Why upper bound and not lower bound?\n",
    "            - Let say the forest consists of N trees.\n",
    "            - By validating on $x_i \\in \\{x_{s}\\}$ with $\\{X_{j_k}\\}$ we notice that $|\\{X_{j_k}\\}| = p \\leq N$.\n",
    "            - Therefore $\\hat{y}_i = \\frac{1}{p}\\sum\\limits_{k=1}^p b_{j_k}(x_i)$, which means that the ensemble got smaller $\\implies$ we average less number of predictions $\\implies$ average error was decrease less.\n",
    "\n",
    "![alt text](https://i.ibb.co/JtB6dV8/Screen-Shot-2020-10-06-at-01-09-31.png)\n",
    "\n",
    "- Random Forest work well with missing values. If we need to predict the target for an instance, whose value for the $j$-th feature is missing, then we can look at the predictions of those trees that didn't use that feature to make the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Decision Boundaries: Random Forest and kNN.\n",
    "\n",
    "![alt text](https://i.ibb.co/rx78Q6t/Screen-Shot-2020-10-06-at-01-38-14.png)\n",
    "\n",
    "- The decision boundaries of Random Forest and kNN look similar. Why is that?\n",
    "- Let's remember:\n",
    "    - kNN finds the $k$ nearest neighbors and the prediction of the instance will be that of their neighbors (the most frequent target). In other words, the prediction is such that the level of similarity of one object with its neighbors is the highest.\n",
    "    - Each tree in the forest divides the feature space into some subspace. Within each subspace will be objects such that the heterogeneity/information criterion is minimal. With entropy, $H(R)$ is minimal if in each subpace there are object from one class. With Gini impurity the same. That is, the criterion in a subpace is minimal if in that subspace we as many elements from one class as possible. That is, when the level of similarity of an objects with its neighbors is the highest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
