{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Classification.\n",
    "\n",
    "---\n",
    "\n",
    "# 1.1. Recap Lecture 2.\n",
    "\n",
    "- Мультиколлинеарность плохо в тем, что при численном обращении матрицы из-за floating point арифметики у нас возникает ошибки, которые накапливются и из-за этого обращение матрицы остановится неточным.\n",
    "- L1 regularization выбирает фичи.\n",
    "\n",
    "---\n",
    "\n",
    "# 1.2. Problem Formulation.\n",
    "\n",
    "Цель: построить такой классификатор $c(\\cdot)$, зависяший от данных, который бы приближал вектор ответов $Y$:\n",
    "\n",
    "![alt text](https://i.ibb.co/NN7Vzbv/Screen-Shot-2020-09-21-at-14-10-37.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 1.3. How to contruct a linear classifier.\n",
    "\n",
    "![alt text](https://i.ibb.co/R29WcrM/Screen-Shot-2020-09-21-at-14-13-52.png)\n",
    "\n",
    "- Why cutoff value is fixed to $0$?\n",
    "    - In Lecture 2 we saw that $x = (x_0, x_1, ..., x_n)$ where $x_0$ is the intercept (bias term). So even is the intercept $x_0 \\neq 0$, by concatenating it to $x$ we consider the model passing through the origin anyway.\n",
    "\n",
    "- Geometrical interpretation ($|C| = 2$): vector $w$ is normal to the hyperplane in $p$-dimensional space (of features). From one side will be objects of one class and from the other side object of the other one.\n",
    "\n",
    "---\n",
    "\n",
    "# 1.4. Margin.\n",
    "\n",
    "For $C = \\{-1, 1\\}$.\n",
    "\n",
    "![alt text](https://i.ibb.co/71W0Nf0/Screen-Shot-2020-09-21-at-14-33-27.png)\n",
    "\n",
    "\n",
    "where:\n",
    "- $f(x_i) = x_i^{\\top}w$ is the prediction for the $i$-th observation.\n",
    "- $y_i$ is the real label (class) for the $i$-th observation.\n",
    "- $\\implies$ if $M_i < 0 \\implies y_i\\cdot f(x_i) < 0 \\implies \\mathrm{sign}({y_i}) \\neq \\mathrm{sign}({f(x_i)}) \\implies$ missclassification.\n",
    "\n",
    "---\n",
    "\n",
    "# 1.5. How to choice weights.\n",
    "\n",
    "![alt text](https://i.ibb.co/hKw3ctg/Screen-Shot-2020-09-21-at-14-47-11.png)\n",
    "\n",
    "\n",
    "Solution to loss not being differentiable: approximate it with a smooth function. Which funtions can we use for this?:\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5.1. Square Loss.\n",
    "\n",
    "![alt text](https://i.ibb.co/BT6HYN2/Screen-Shot-2020-09-21-at-17-20-00.png)\n",
    "\n",
    "- По условию $y_i^2 = 1$, since $y_i \\in \\{-1, 1\\}$.\n",
    "\n",
    "\n",
    "- **Disadvantage of MSE:**\n",
    "    - Penalize for high condidence: looking at the graph we see that this loss not only penalizes when we misclassify some instance ($M_i < 1$), but also when when $M_i > 1$. That is, when are prediction not only is correct but too confident, then we also get penalized because of that.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "## 1.5.2. Other Losses.\n",
    "\n",
    "![alt text](https://i.ibb.co/F0yVcMf/Screen-Shot-2020-09-21-at-17-41-00.png)\n",
    "\n",
    "\n",
    "- Different losses (after minimization) give different solutions (different target predictions).\n",
    "    - Каждый лозз порождает свой классификатор.\n",
    "    \n",
    "\n",
    "From the graph we see that there are losses that don't have the disadvantage of MSE. Let's study some of them.\n",
    "\n",
    "---\n",
    "\n",
    "# 1.6. Logistic Regression.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6.1. Intuition.\n",
    "\n",
    "![alt text](https://i.ibb.co/PwcYgFJ/Screen-Shot-2020-09-21-at-18-21-06.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6.2. Sigmoid Function.\n",
    "\n",
    "![alt text](https://i.ibb.co/qR5STFC/Screen-Shot-2020-09-21-at-18-35-01.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6.3. Как мы ставим задачу оптимизации?\n",
    "\n",
    "- For this we find the principle of Maximum Likelihood Estimation.\n",
    "\n",
    "![alt text](https://i.ibb.co/4PJvWkq/Screen-Shot-2020-09-21-at-18-39-56.png)\n",
    "\n",
    "$$-\\log L(w\\mid X,Y) \\to \\min_w $$\n",
    "\n",
    "$\\implies \\ell = -\\log L(w\\mid X,Y)$ is a loss.\n",
    "\n",
    "![alt text](https://i.ibb.co/khmNk77/Screen-Shot-2020-09-22-at-13-57-22.png)\n",
    "\n",
    "- It penalizes misclassifcations.\n",
    "- The more confident we are in our correct classification, the less it penalizes it. \n",
    "\n",
    "----\n",
    "\n",
    "## 1.7. Probability Calibration.\n",
    "\n",
    "- A probabilistic model is calibrated if I binned the test samples based on their predicted probabilities, each bin’s true outcomes has a proportion close to the probabilities in the bin.\n",
    "- Calibration plots are often line plots. Once I choose the number of bins and throw predictions into the bin, each bin is then converted to a dot on the plot. For each bin, the $y$-value is the proportion of true outcomes in the bin, and $x$-value is the mean predicted probability.\n",
    "\n",
    "![alt text](https://i.ibb.co/gmZrCvV/Screen-Shot-2020-09-22-at-14-23-40.png)\n",
    "![alt text](https://i.ibb.co/S5H1gVX/Screen-Shot-2020-09-22-at-14-25-43.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1.8. Multiclass Aggregation Strategies.\n",
    "\n",
    "- Выше не могли никак рассмотреть мультиклассовый случай, потому что мы линейной моделью предсказываем одно число и одно число только дает один порог, который делит гиперплоскоть только 2 части.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1.8.1 One vs. Rest.\n",
    "\n",
    "1. We take one class and make it the positive class.\n",
    "2. Make all the remaining objects belong to the negative class.\n",
    "3. Apply the model above.\n",
    "4. Take another class and go to step 1.\n",
    "\n",
    "In this fashion we end up training $N$ classifiers, where $N$ is the number of classes.\n",
    "\n",
    "For classification of a point, we find the probability wrt all the classifiers and choose the class, whose classifier returns the highest probability for that point.\n",
    "\n",
    "![alt text](https://i.ibb.co/4VnNT8N/Screen-Shot-2020-09-22-at-14-32-06.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 1.8.1. Disadvantages:\n",
    "\n",
    "**Not alway works:**\n",
    "\n",
    "- Какой датасет можно предложить, форма которого сломает этот классиыикатор?\n",
    "    - Это когда центр каждого класса расположен на одной прямой.\n",
    "        - Крайний хорош отлелаются а центральный нет, так как с линейным классификатором нельзя отделятся от более одного класса (2 соседних).\n",
    "        \n",
    "** Appear unclassified regions:**\n",
    "\n",
    "![alt text](https://i.ibb.co/c1Qw8Bs/Screen-Shot-2020-09-22-at-14-56-49.png)\n",
    "\n",
    "The solutions to this problem:\n",
    "\n",
    "![alt text](https://i.ibb.co/dmbRM7b/Screen-Shot-2020-09-22-at-14-58-17.png)\n",
    "\n",
    "**<font color=red>How to achieve the shaded regions? Find out</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.8.2. One vs. One.\n",
    "\n",
    "- We get $\\frac{N(N-1)}{2}$ classifiers.\n",
    "![alt text](https://i.ibb.co/B2xrNzk/Screen-Shot-2020-09-22-at-15-03-31.png)\n",
    "\n",
    "---\n",
    "\n",
    "# 1.9. Binary Classification Metrics.\n",
    "\n",
    "- In regression most of metric are also taken as loss functions, since most of them are differentiable.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.9.1. Accuracy.\n",
    "\n",
    "![alt text](https://i.ibb.co/zNJqzL8/Screen-Shot-2020-09-22-at-15-08-26.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.9.2. Precision and Recall.\n",
    "\n",
    "![alt text](https://i.ibb.co/rGGp6r3/Screen-Shot-2020-09-22-at-15-13-26.png)\n",
    "\n",
    "$$\\mathrm{Precision} = \\frac{\\text{correctly classified as positive}}{\\text{all classified as positive}}$$\n",
    "\n",
    "$$\\mathrm{Recall} = \\frac{\\text{correctly classified as positive}}{\\text{all the real positives}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1.9.3. $F$ Score.\n",
    "\n",
    "- We would want to maximize both Precision and Recall.\n",
    "- Usually one of these two metric is better (higher) than the other.\n",
    "- $\\implies$ let's maximize the worst of them, that is, $\\min\\{\\text{Precision}, \\text{Recall}\\}$, but this function is not differentiable.\n",
    "\n",
    "![alt text](https://i.ibb.co/hXBWX0F/Screen-Shot-2020-09-22-at-15-28-45.png)\n",
    "\n",
    "We can approximate with a smooth function: We can take the harmonic mean of both functions:\n",
    "\n",
    "![alt text](https://i.ibb.co/XJQkk5K/Screen-Shot-2020-09-22-at-15-29-37.png)\n",
    "\n",
    "![alt text](https://i.ibb.co/kS5rj5d/Screen-Shot-2020-09-22-at-15-30-57.png)\n",
    "\n",
    "Usually make a II type error is worse than making a I type error (telling an ill-person that is healthy). That is, kind of Recall is more important than Precision in that case, that want take into account $F_{\\beta},\\ \\beta \\in (0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.9.4. ROC.\n",
    "\n",
    "![alt text](https://i.ibb.co/QDNrp2p/Screen-Shot-2020-09-22-at-15-36-57.png)\n",
    "\n",
    "![alt text](https://i.ibb.co/zb8vSN1/Screen-Shot-2020-09-22-at-15-40-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Baseline: is the diagonal line (random predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.9.5. ROC-AUC.\n",
    "\n",
    "![alt text](https://i.ibb.co/XL52xs8/Screen-Shot-2020-09-22-at-15-47-36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 1.9.6. Precision-Recall Curve.\n",
    "\n",
    "![alt text](https://i.ibb.co/ySF2jmb/Screen-Shot-2020-09-22-at-15-52-23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1.10. Multiclass Classification Metrics.\n",
    "\n",
    "- **Read [here](https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel) and [here](https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification).**\n",
    "\n",
    "![alt text](https://i.ibb.co/bmBGtsQ/Screen-Shot-2020-09-22-at-15-53-22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.10.1. Confusion Matrix.\n",
    "\n",
    "- On x-axis: predicted target.\n",
    "- On y-axis: real target.\n",
    "- For a perfect classifier we'll get only values along the diagonal.\n",
    "\n",
    "![alt text](https://i.ibb.co/X8Bpr4G/Screen-Shot-2020-09-22-at-16-00-47.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
